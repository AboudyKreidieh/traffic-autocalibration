{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" Contains Progressive Neural Network Class\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "NUM_SGD_STEPS = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTaskNetwork:\n",
    "    \"\"\"\n",
    "    Creates a NN for single task\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, size, activations, input_ph, col_num, dropout=False, batch_norm=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: list of ints\n",
    "            list of units in hidden layers and output layer \n",
    "            The last element should be the output size, example: [128,128,1]\n",
    "        activations: list of tf.nn activations\n",
    "            list of activation functions for each layer, \n",
    "            Size of the list should be number of hidden layers + 1, \n",
    "            the last element should be the output activation function, example: [tf.nn.relu,tf.nn.relu, None]\n",
    "        input_ph: tf placeholder\n",
    "            TensorFlow placeholder for the inputs\n",
    "        dropout: boolean, optional\n",
    "            specifies whether to use dropout\n",
    "        batch_norm: boolean, optional\n",
    "            specifies whether to use batch normalization\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.activations = activations\n",
    "        self.input_ph = input_ph\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.h = [self.input_ph]\n",
    "        self.col_num = col_num\n",
    "\n",
    "    def create_single_task_nn(self):\n",
    "        \"\"\" Creates a the neural network for regression problem\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        output_layer: Output layer prediction\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        last_layer = self.input_ph\n",
    "\n",
    "        scope = \"nn\"\n",
    "        if self.dropout:\n",
    "            scope += \"_d\"\n",
    "        if self.batch_norm:\n",
    "            scope += \"bn\"\n",
    "            \n",
    "        # create variables\n",
    "        for layer in range(len(self.size)):\n",
    "            if layer == 0:  # input layer                \n",
    "                self.weights.append(tf.get_variable(\n",
    "                    name='W{}_{}'.format(layer, self.col_num),\n",
    "                    shape=[self.input_ph.shape[1], self.size[0]], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "                ))\n",
    "                self.biases.append(tf.get_variable(\n",
    "                    name='b{}_{}'.format(layer, self.col_num), \n",
    "                    shape=[self.size[0]], \n",
    "                    initializer=tf.constant_initializer(0.)\n",
    "                ))\n",
    "            else:   # hidden and output layers:\n",
    "                shape = self.size[layer-1:layer+1]\n",
    "                self.weights.append(tf.get_variable(\n",
    "                    name='W{}_{}'.format(layer, self.col_num), \n",
    "                    shape=shape, \n",
    "                    initializer=tf.contrib.layers.xavier_initializer()\n",
    "                ))\n",
    "                self.biases.append(tf.get_variable(\n",
    "                    name='b{}_{}'.format(layer, self.col_num), \n",
    "                    shape=shape[1], \n",
    "                    initializer=tf.constant_initializer(0.)\n",
    "                ))\n",
    "\n",
    "        # create computation graph\n",
    "        i = 0\n",
    "        for W, b, activation in zip(self.weights, self.biases, self.activations):\n",
    "            last_layer = tf.matmul(last_layer, W) + b\n",
    "            self.h.append(last_layer)\n",
    "            if activation is not None:\n",
    "                last_layer = activation(last_layer)\n",
    "            if self.dropout:\n",
    "                last_layer = tf.nn.dropout(last_layer, 0.5)\n",
    "            if self.batch_norm:\n",
    "                last_layer = tf.contrib.layers.batch_norm(last_layer, \n",
    "                                                          center=True, scale=True, \n",
    "                                                          scope=\"{}_{}\".format(scope,i), \n",
    "                                                          reuse=tf.AUTO_REUSE)\n",
    "            i += 1\n",
    "        self.output_pred = last_layer        \n",
    "        \n",
    "        return self.output_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveNN:\n",
    "    \"\"\"\n",
    "    Creates a progressive neural network. The last column will be the target task column.\n",
    "    Other columns represent source tasks\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, size, activations, input_ph, num_source_cols, dropout=False, batch_norm=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: list of ints\n",
    "            list of units in hidden layers and output layer \n",
    "            The last element should be the output size, example: [128,128,1]\n",
    "        activations: list of tf.nn activations\n",
    "            list of activation functions for each layer, \n",
    "            Size of the list should be number of hidden layers + 1, \n",
    "            the last element should be the output activation function, example: [tf.nn.relu,tf.nn.relu, None]\n",
    "        input_ph: tf placeholder\n",
    "            TensorFlow placeholder for the inputs\n",
    "        num_source_cols: int, \n",
    "            Number of source task columns. \n",
    "        dropout: boolean, optional\n",
    "            specifies whether to use dropout\n",
    "        batch_norm: boolean, optional\n",
    "            specifies whether to use batch normalization\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.activations = activations\n",
    "        self.input_ph = input_ph\n",
    "        self.num_source_cols = num_source_cols\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.last_layer = self.input_ph\n",
    "        self.num_layers = len(self.size)\n",
    "        self.num_cols = self.num_source_cols + 1  # 1 is the target domain net\n",
    "        self.source_weights = [[] for _ in range(self.num_source_cols)] \n",
    "        self.source_biases = [[] for _ in range(self.num_source_cols)]\n",
    "        self.latteral_connectins = []\n",
    "        self.latteral_conns = []\n",
    "        for l in range(self.num_layers):\n",
    "            self.latteral_conns.append( [[]] * self.num_cols )\n",
    "        self.latteral_conns[0] = None\n",
    "        self.target_weights = []\n",
    "        self.target_biases = []\n",
    "        \n",
    "\n",
    "    def create_progressive_nn(self):\n",
    "        \"\"\" Creates a the neural network for regression problem\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        output_layer: Output layer prediction\n",
    "        \"\"\"\n",
    "        scope = \"nn\"\n",
    "        if self.dropout:\n",
    "            scope += \"_d\"\n",
    "        if self.batch_norm:\n",
    "            scope += \"bn\"\n",
    "   \n",
    "        # create weights and biases\n",
    "        col_objs = [SingleTaskNetwork(self.size, self.activations, self.input_ph, col)\n",
    "                    for col in range(self.num_cols)]\n",
    "        for col_obj in col_objs:\n",
    "            col_obj.create_single_task_nn()\n",
    "            \n",
    "        # create connections and computation graph\n",
    "        if self.num_cols == 1:\n",
    "            return col_objs[0].output_pred\n",
    "        \n",
    "        last_layer = [self.input_ph for _ in range(self.num_cols)] \n",
    "      \n",
    "        for layer in range(self.num_layers):   \n",
    "            for col in range(self.num_cols): \n",
    "                last_layer[col] = tf.matmul(last_layer[col], col_objs[col].weights[layer]) + col_objs[col].biases[layer]\n",
    "                if col > 0 and layer > 0:\n",
    "                    for c in range(col):\n",
    "                        U_shape = [col_objs[c].size[layer-1], col_objs[col].size[layer]]  # from layer l-1 of all prev columns\n",
    "                        new_U = tf.get_variable(name='U_{}_{}_{}'.format(layer,col,c),   \n",
    "                                                shape=U_shape, \n",
    "                                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "                        self.latteral_conns[layer][col].append(new_U)\n",
    "                        last_layer[col] += tf.matmul(col_objs[c].h[layer-1],new_U)\n",
    "                        \n",
    "                if col_objs[col].activations[layer] is not None:\n",
    "                    last_layer[col] = col_objs[col].activations[layer](last_layer[col])\n",
    "                if self.dropout:\n",
    "                    last_layer[col] = tf.nn.dropout(last_layer[col], 0.5)\n",
    "                if self.batch_norm:\n",
    "                    last_layer[col] = tf.contrib.layers.batch_norm(last_layer[col], \n",
    "                                                              center=True, scale=True, \n",
    "                                                              scope=\"{}_{}\".format(scope,i), \n",
    "                                                              reuse=tf.AUTO_REUSE)\n",
    "                col_objs[col].h[layer] = last_layer[col]\n",
    "        output_pred = last_layer[col]        \n",
    "        \n",
    "        return output_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# state and action placeholders\n",
    "output_ph = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "input_ph = tf.placeholder(dtype=tf.float32, shape=[None, train_set.shape[1]-2])\n",
    "\n",
    "# session\n",
    "sess = tf.Session()\n",
    "\n",
    "# function approximator\n",
    "# func_approx = create_nn(input_ph, dropout=True)\n",
    "size = [128,1]\n",
    "activations = [tf.nn.leaky_relu,None]\n",
    "num_source_cols = 1\n",
    "obj = ProgressiveNN(size, activations, input_ph, num_source_cols)\n",
    "func_approx = obj.create_progressive_nn()\n",
    "# loss\n",
    "loss = tf.reduce_mean(0.5 * tf.square(output_ph - func_approx))\n",
    "\n",
    "\n",
    "learning_rate = [0.01]  #[12e-3, 11e-3, 9e-3, 0.01]\n",
    "\n",
    "res_dropout = []\n",
    "min_error = float(\"inf\")\n",
    "\n",
    "for i in learning_rate:  # grid search\n",
    "    res_dropout.append([])\n",
    "    # optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate= i).minimize(loss)\n",
    "\n",
    "    # initialize all the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for j in range(NUM_SGD_STEPS):  # train for 1000 SGD steps\n",
    "        # collect a batch of data\n",
    "        indices = np.random.randint(low=0, \n",
    "                                    high=train_set.shape[0],\n",
    "                                    size=BATCH_SIZE)\n",
    "        obs = train_set[indices,1:-1]\n",
    "        actions = train_set[indices,-1:]\n",
    "\n",
    "        # compute the loss/return and update the weights of the policy\n",
    "        _ = sess.run(opt, feed_dict={input_ph: obs, output_ph: actions})\n",
    "\n",
    "        # compute the loss on the test set\n",
    "        mse = sess.run(loss, feed_dict={input_ph: test_set[:,1:-1],\n",
    "                                        output_ph: test_set[:,-1:]})\n",
    "\n",
    "        res_dropout[-1].append(mse)\n",
    "        \n",
    "        if mse < min_error:\n",
    "            min_error = mse\n",
    "            best_learning_rate = i\n",
    "            \n",
    "        print(i, j, mse)\n",
    "print (\"best_learning_rate: %s\" % best_learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow)",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
